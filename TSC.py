#!/usr/bin/env python
# coding: utf-8

# # **Caso pràctico: Selecciòn de Caracteristicas.**
# 
# En este Noteboock se analizarà un caso pràctico que representa unmecanismo de seleccion de caracteristicas mediante el uso de Eandom Forest.

# ## Descripciòn:
# 
# The sophisticated and advanced Android malware is able to identify the presence of the emulator used by the malware analyst and in response, alter its behaviour to evade detection. To overcome this issue, we installed the Android applications on the real device and captured its network traffic. 
# 
# CICAAGM dataset is captured by installing the Android apps on the real smartphones semi-automated. The dataset is generated from 1,900 applications with the following three categories:
# 
# **1. Adware (250 apps)**
# 
# Airpush: Designed to deliver unsolicited advertisements to the user’s systems for information stealing.
# 
# * **Dowgin:** Designed as an advertisement library that can also steal the user’s information.
# 
# * **Kemoge:** Designed to take over a user’s Android device. This adware is a hybrid of botnet and disguises itself as popular apps via repackaging.
# 
# * **Mobidash:** Designed to display ads and to compromise user’s personal information.
# 
# * **Shuanet:** Similar to Kemoge, Shuanet is also designed to take over a user’s device.
# 
# **2. General Malware (150 apps)**
# 
# * **AVpass:** Designed to be distributed in the guise of a Clock app.
# 
# * **FakeAV:** Designed as a scam that tricks user to purchase a full version of the software in order to re-mediate non-existing infections.
# 
# * **FakeFlash/FakePlayer:** Designed as a fake Flash app in order to direct users to a website (after successfully installed).
# 
# * **GGtracker:** Designed for SMS fraud (sends SMS messages to a premium-rate number) and information stealing.
# 
# * **Penetho:** Designed as a fake service (hacktool for Android devices that can be used to crack the WiFi password). The malware is also able to infect the user’s computer via infected email attachment, fake updates, external media and infected documents.
# 
# **3. Benign (1,500 apps)**
# 
# * 2015 GooglePlay market (top free popular and top free new)
# 
# * 2016 GooglePlay market (top free popular and top free new)
# 
# **License**
# 
# The CICAAGM dataset consists of the following items is publicly available for researchers.
# 
# * .pcap files – the network traffic of both the malware and benign (20% malware and 80% benign)
# 
# * .csv files - the list of extracted network traffic features generated by the CIC-flowmeter
# 
# If you are using our dataset, you should cite our related paper that outlines the details of the dataset and its underlying principles:
# 
# Arash Habibi Lashkari, Andi Fitriah A. Kadir, Hugo Gonzalez, Kenneth Fon Mbah and Ali A. Ghorbani, “Towards a Network-Based Framework for Android Malware Detection and Characterization”, In the proceeding of the 15th International Conference on Privacy, Security and Trust, PST, Calgary, Canada, 2017.

# ## **Imports**

# In[1]:


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import f1_score


# ## **Funciones Auxiliares** 

# In[2]:


# Construcción de una función que realize el particionado completo.
def train_val_test_split(df, rstate=42, shuffle=True, stratify=None):
    strat = df[stratify] if stratify else None 
    train_set, test_set = train_test_split(
        df, test_size = 0.4, random_state = rstate, shuffle = shuffle, stratify = strat)
    strat = train_test[stratify] if stratify else None
    val_set, test_set = train_test_split(
        test_set, test_size=0.5, random_state = rstate, shuffle=shuffle, stratify = strat)
    return (train_set, val_set, test_set)


# In[3]:


def remove_labels(df, label_name):
    X = df.drop(label_name, axis=1)
    y = df[label_name].copy()
    return (X,y)


# ## **1.- Lectura del DataSet de Datos**

# In[4]:


df = pd.read_csv('AndroiDataSet/AndroidAdware2017/TotalFeatures-ISCXFlowMeter.csv')


# ## **2.- Visualizacion del DataSet**

# In[5]:


df.head(10)


# In[6]:


df.describe()


# In[7]:


df.info()


# ## **3.- Divisiòn del DataSet**

# In[8]:


# Dividimos el DataSet
train_set, val_set, test_set = train_val_test_split(df)


# In[9]:


X_train, y_train = remove_labels(train_set, 'calss')
X_val, y_val = remove_labels(val_set, 'calss')
X_test, y_test = remove_labels(test_set, 'calss')


# ## **4.- Random Forest**

# In[10]:


from sklearn.ensemble import RandomForestClassifier

# Modelo  entrenado con el DataSet sin escalar
clf_rnd = RandomForestClassifier(n_estimators=100, random_state = 42, n_jobs = -1)
clf_rnd.fit(X_train, y_train)


# In[11]:


# Predecir con el DataSet de validacion
y_pred = clf_rnd.predict(X_val)


# In[12]:


print("F1 Score: ", f1_score(y_pred, y_val, average="weighted"))

# ## **5.- Seleccion del modelo**
# 
# * Tanto los arboles de decision como los conjuntos de arboles(al igual que otros algoritmos mas complejos) representan un gran numero de hiperparametros que deben evaluarse para conseguir el mejor modelo. Una de las formas mas comunes de seleccionarlo es mediante tecnicas de seleccion del modelo.

# In[13]:


# Uso sw Grid Search para seleccion del modelo

from sklearn.model_selection import GridSearchCV

param_grid = [
    #Try 9 (3x3) combinations of hyperparameters
    {'n_estimators': [10, 5, 10], 'max_leaf_nodes': [1, 2, 3]},
    # they try 2 (2x3) combinations whit bootsrap set as False
    {'bootstrap': [False], 'n_estimators': [10, 50]},
    ]
rnd_clf = RandomForestClassifier(n_jobs=-1, random_state=42)

#train across 5 folds, that's a total of (9+6)*5=75 round of training
grid_search = GridSearchCV(rnd_clf, param_grid, cv=5,
                          scoring='f1_weighted', return_train_score = True)
grid_search.fit(X_train, y_train)


# In[14]:


grid_search.best_params_


# In[15]:


grid_search.best_estimator_


# In[16]:


cvres= grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print("F1 Score:", mean_score, "-", "Parametros:", params)


# #### Es importante entender que hay parametros, como las combinaciones de las caracteristicas que hemos visto en otros ejercicios, que pueden tratarse como hiperparametros y evaluarse de la misma manera.
# 
# #### Esta manera que se ha programado, sirve para explorar hiperparametros, esta bien si no se tiene una gran cantidad de combinaciones y se tiene bien definidos los posibles valores. En caso contrario es posible eficientar utilizando  **RandomizeSearchCV** que funciona de manera similar al caso anterior, pero realizando una busqueda sobre valores randomizados. 

# In[17]:


from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint


# In[18]:


params_distribs = {
    'n_estimators': randint(low=1, high=200),
    'max_depth': randint(low=8, high=50),
}

rnd_clf  = RandomForestClassifier(n_jobs=-1)

# Train cross 2 folds, that's a total of 5*2=10 round of training

rnd_search = RandomizedSearchCV(rnd_clf, param_distributions=params_distribs,
                               n_iter=5, cv=2, scoring="f1_weighted")


rnd_search.fit(X_train, y_train)

